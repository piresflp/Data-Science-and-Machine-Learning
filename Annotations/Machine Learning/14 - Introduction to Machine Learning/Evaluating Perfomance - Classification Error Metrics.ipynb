{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* After out machine learning process is complete, we will use performance metrics to evaluate how our model did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The key classification metrics we need to understand are: Accuracy, Recal, Precision and F1-Score\n",
    "\n",
    "* All classification metrics extends from the ideia that your model can only be correct or incorrect.\n",
    "\n",
    "* At the end of the tests, we will have a count of correct matches and a count of incorrect matches. But, in the real world, not all incorrect or correct matches hold equal value! Thus, a single metric won't tell the complete story!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Accuracy\n",
    "*  Accuracy in classification problems is the number of correct predictions made by the model divided by the total number of predictions. (c/t)\n",
    "*  Accuracy is useful when target classes are well balanced. (if we have roughly the same amount of correct and incorrect examples)\n",
    "*  Accuracy is <strong>not</strong> a good choice with unbalanced classes! (in this situation we'll want to understand <strong>recall</strong> and <strong>precision</strong>)\n",
    "\n",
    "### * Recall\n",
    "*  Ability of a model to find all the relevant cases within a dataset.\n",
    "*  The precise definition of recall is the number of true positives <strong>divided by</strong> the number of true positives plus the number os false negatives\n",
    "\n",
    "### * Precision\n",
    "*  Ability of a classification model to identify only the relevant data points.\n",
    "*  Precision is defined as the number of true positives divided by the number of true positives plus the number of false negatives\n",
    "\n",
    "### * Recall and Precision\n",
    "*  While recall expresses the ability to find all relevant instances in a dataset, precision expresses the proportion of the data points our model says was relevant actually were relevant.\n",
    "\n",
    "### * F1-Score\n",
    "*  An optimal blend of precision and recall, combining both metrics.\n",
    "*  F1 score is the harmonic mean of precision and recall taking both metrics.\n",
    "\n",
    "*  <strong>F1 = 2*(precision * recall)/(precision+recall)</strong>\n",
    "\n",
    "*  The use of harmonic mean instead of a simple average punishes extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
